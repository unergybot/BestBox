#!/usr/bin/env python3
"""
Adaptive Troubleshooting Searcher

Multi-stage search with LLM-based query classification:
1. Classify query (case-level vs issue-level vs hybrid)
2. Vector search with BGE-M3
3. Rerank with BGE-reranker-v2-m3
4. Metadata boosting

Usage:
    from services.troubleshooting.searcher import TroubleshootingSearcher

    searcher = TroubleshootingSearcher()
    results = searcher.search("äº§å“æŠ«é”‹è§£å†³æ–¹æ¡ˆ", top_k=5)
"""

import sys
from pathlib import Path

# Add project root to path when running as script
if __name__ == "__main__":
    project_root = Path(__file__).parent.parent.parent
    sys.path.insert(0, str(project_root))

from qdrant_client import QdrantClient
from qdrant_client.models import Filter, FieldCondition, MatchValue, MatchAny
import requests
from typing import List, Dict, Literal, Optional
import json
import logging

from services.troubleshooting.embedder import TroubleshootingEmbedder

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

SearchMode = Literal["CASE_LEVEL", "ISSUE_LEVEL", "HYBRID"]


class TroubleshootingSearcher:
    """Adaptive multi-level search for troubleshooting knowledge base"""

    def __init__(
        self,
        llm_url: str = "http://localhost:8080",
        embeddings_url: str = "http://localhost:8081",
        reranker_url: str = "http://localhost:8082",
        qdrant_host: str = "localhost",
        qdrant_port: int = 6333
    ):
        """Initialize searcher with service URLs"""

        self.llm_url = llm_url
        self.embeddings_url = embeddings_url
        self.reranker_url = reranker_url

        self.qdrant = QdrantClient(host=qdrant_host, port=qdrant_port)
        self.embedder = TroubleshootingEmbedder(embeddings_url=embeddings_url)

        logger.info("Searcher initialized")

    def search(
        self,
        query: str,
        top_k: int = 5,
        filters: Optional[Dict] = None,
        classify: bool = True
    ) -> Dict:
        """
        Main search entry point with adaptive routing.

        Args:
            query: Search query
            top_k: Number of results to return
            filters: Optional metadata filters
            classify: Whether to use LLM classification (disable for testing)

        Returns:
            dict with query, mode, results, total_found
        """
        logger.info(f"ğŸ” Searching: \"{query}\"")

        # Step 1: Classify query intent (if enabled)
        if classify:
            mode = self._classify_query(query)
        else:
            mode = "ISSUE_LEVEL"  # Default for testing

        logger.info(f"   Mode: {mode}")

        # Step 2: Route to appropriate search strategy
        if mode == "CASE_LEVEL":
            results = self._search_cases(query, top_k, filters)

        elif mode == "ISSUE_LEVEL":
            results = self._search_issues(query, top_k, filters)

        else:  # HYBRID
            case_results = self._search_cases(query, max(1, top_k // 2), filters)
            issue_results = self._search_issues(query, top_k, filters)
            results = self._merge_results(case_results, issue_results, top_k)

        logger.info(f"   Found: {len(results)} results")

        return {
            "query": query,
            "mode": mode,
            "results": results,
            "total_found": len(results)
        }

    def _classify_query(self, query: str) -> SearchMode:
        """
        Use LLM to determine search granularity.

        Args:
            query: User query

        Returns:
            Search mode (CASE_LEVEL, ISSUE_LEVEL, or HYBRID)
        """
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæœç´¢æ„å›¾åˆ†ç±»å™¨ã€‚åˆ†æç”¨æˆ·æŸ¥è¯¢å¹¶ç¡®å®šæœç´¢ç²’åº¦ã€‚

ç”¨æˆ·æŸ¥è¯¢: "{query}"

åˆ¤æ–­æ ‡å‡†:
- CASE_LEVEL: æŸ¥è¯¢æ•´ä¸ªæ¡ˆä»¶çš„ä¿¡æ¯
  ä¾‹å¦‚: "é›¶ä»¶1947688çš„æ‰€æœ‰é—®é¢˜", "HIPSææ–™çš„æ¡ˆä¾‹", "2025å¹´9æœˆçš„æ¡ˆä»¶"

- ISSUE_LEVEL: æŸ¥è¯¢ç‰¹å®šé—®é¢˜æˆ–è§£å†³æ–¹æ¡ˆ
  ä¾‹å¦‚: "äº§å“æŠ«é”‹çš„è§£å†³æ–¹æ³•", "æ¨¡å…·è¡¨é¢æ±¡æŸ“", "ç«èŠ±çº¹é—®é¢˜"

- HYBRID: éœ€è¦ä¸¤ä¸ªå±‚çº§çš„ä¿¡æ¯
  ä¾‹å¦‚: "æŠ«é”‹é—®é¢˜çš„æ¡ˆä¾‹æœ‰å“ªäº›", "æ˜¾ç¤ºæ‰€æœ‰T1æˆåŠŸçš„è§£å†³æ–¹æ¡ˆ"

åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦å…¶ä»–è¯´æ˜:
{{"mode": "CASE_LEVEL|ISSUE_LEVEL|HYBRID", "confidence": 0.0-1.0, "reasoning": "ç®€çŸ­è§£é‡Š"}}"""

        try:
            response = requests.post(
                f"{self.llm_url}/v1/chat/completions",
                json={
                    "model": "qwen3",
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.1,
                    "max_tokens": 100
                },
                timeout=10
            )

            result = response.json()
            content = result['choices'][0]['message']['content']

            # Extract JSON from response
            json_start = content.find('{')
            json_end = content.rfind('}') + 1

            if json_start >= 0 and json_end > json_start:
                classification = json.loads(content[json_start:json_end])
                mode = classification.get('mode', 'ISSUE_LEVEL')

                if mode in ["CASE_LEVEL", "ISSUE_LEVEL", "HYBRID"]:
                    return mode

        except Exception as e:
            logger.warning(f"Query classification failed: {e}, using ISSUE_LEVEL")

        # Fallback to ISSUE_LEVEL (most common)
        return "ISSUE_LEVEL"

    def _search_cases(
        self,
        query: str,
        top_k: int,
        filters: Optional[Dict] = None
    ) -> List[Dict]:
        """Case-level search"""

        # Get query embedding
        query_embedding = self.embedder._get_embedding(query)

        # Build Qdrant filter
        qdrant_filter = self._build_filter(filters) if filters else None

        # Search using query_points
        response = self.qdrant.query_points(
            collection_name="troubleshooting_cases",
            query=query_embedding,
            query_filter=qdrant_filter,
            limit=top_k,
            score_threshold=0.5,
            with_payload=True
        )

        results = response.points

        # Format results
        formatted_results = []
        for hit in results:
            formatted_results.append({
                "type": "case",
                "score": float(hit.score),
                "case_id": hit.payload['case_id'],
                "part_number": hit.payload.get('part_number'),
                "material": hit.payload.get('material'),
                "total_issues": hit.payload.get('total_issues'),
                "source_file": hit.payload.get('source_file'),
                "text_summary": hit.payload.get('text_summary')
            })

        return formatted_results

    def _search_issues(
        self,
        query: str,
        top_k: int,
        filters: Optional[Dict] = None
    ) -> List[Dict]:
        """Issue-level search with reranking"""

        # Stage 1: Vector search (retrieve 3x for reranking)
        query_embedding = self.embedder._get_embedding(query)
        qdrant_filter = self._build_filter(filters) if filters else None

        response = self.qdrant.query_points(
            collection_name="troubleshooting_issues",
            query=query_embedding,
            query_filter=qdrant_filter,
            limit=top_k * 3,
            score_threshold=0.4,
            with_payload=True
        )

        candidates = response.points

        if not candidates:
            return []

        # Stage 2: Rerank (if reranker is available)
        try:
            reranked = self._rerank(query, candidates, top_k)
        except Exception as e:
            logger.warning(f"Reranking failed: {e}, using vector scores")
            reranked = [
                {"score": float(c.score), "payload": c.payload}
                for c in candidates[:top_k]
            ]

        # Stage 3: Metadata boosting
        final_results = []
        for item in reranked:
            score = item['score']
            payload = item['payload']

            # Boost successful solutions
            if payload.get('result_t1') == 'OK' or payload.get('result_t2') == 'OK':
                score *= 1.15

            # Boost if part number mentioned in query
            if payload.get('part_number') and str(payload['part_number']) in query:
                score *= 1.3

            final_results.append({
                "type": "issue",
                "score": float(score),
                "issue_id": payload.get('issue_id'),
                "case_id": payload.get('case_id'),
                "part_number": payload.get('part_number'),
                "issue_number": payload.get('issue_number'),
                "trial_version": payload.get('trial_version'),
                "category": payload.get('category'),
                "problem": payload.get('problem'),
                "solution": payload.get('solution'),
                "result_t1": payload.get('result_t1'),
                "result_t2": payload.get('result_t2'),
                "images": payload.get('images', []),
                "defect_types": payload.get('defect_types', [])
            })

        # Re-sort by boosted scores
        final_results.sort(key=lambda x: x['score'], reverse=True)

        return final_results[:top_k]

    def _rerank(self, query: str, candidates: List, top_k: int) -> List:
        """Rerank candidates using BGE-reranker-v2-m3"""

        # Prepare documents for reranking
        documents = []
        for candidate in candidates:
            # Combine text fields
            doc_text = f"{candidate.payload.get('problem', '')} {candidate.payload.get('solution', '')}"

            # Add image descriptions if available
            for img in candidate.payload.get('images', []):
                if img.get('vl_description'):
                    doc_text += f" {img['vl_description']}"

            documents.append(doc_text)

        # Call reranker service
        response = requests.post(
            f"{self.reranker_url}/rerank",
            json={
                "query": query,
                "documents": documents,
                "top_k": top_k
            },
            timeout=30
        )

        response.raise_for_status()
        rerank_results = response.json()['results']

        # Map back to candidates
        reranked = []
        for item in rerank_results:
            idx = item['index']
            reranked.append({
                "score": item['score'],
                "payload": candidates[idx].payload
            })

        return reranked

    def _merge_results(
        self,
        case_results: List[Dict],
        issue_results: List[Dict],
        top_k: int
    ) -> List[Dict]:
        """Merge and deduplicate case + issue results"""

        # Combine results with type marker
        all_results = case_results + issue_results

        # Sort by score
        all_results.sort(key=lambda x: x['score'], reverse=True)

        # Return top_k
        return all_results[:top_k]

    def _build_filter(self, filters: Dict) -> Filter:
        """Build Qdrant filter from dict"""

        conditions = []

        if 'part_number' in filters:
            conditions.append(
                FieldCondition(
                    key="part_number",
                    match=MatchValue(value=filters['part_number'])
                )
            )

        if 'trial_version' in filters:
            conditions.append(
                FieldCondition(
                    key="trial_version",
                    match=MatchValue(value=filters['trial_version'])
                )
            )

        if 'result' in filters:
            # Match either result_t1 or result_t2
            conditions.append(
                FieldCondition(
                    key="result_t1",
                    match=MatchValue(value=filters['result'])
                )
            )

        return Filter(must=conditions) if conditions else None


if __name__ == "__main__":
    # Test search
    print("Testing Troubleshooting Searcher")
    print("=" * 70)
    print()

    searcher = TroubleshootingSearcher()

    # Test queries
    test_queries = [
        "äº§å“æŠ«é”‹",
        "æ¨¡å…·è¡¨é¢æ±¡æŸ“é—®é¢˜",
        "é›¶ä»¶1947688",
    ]

    for query in test_queries:
        print(f"Query: \"{query}\"")
        print("-" * 70)

        results = searcher.search(query, top_k=3, classify=False)

        print(f"Mode: {results['mode']}")
        print(f"Found: {results['total_found']} results\n")

        for i, result in enumerate(results['results'][:2], 1):
            print(f"{i}. [{result['type']}] Score: {result['score']:.3f}")
            if result['type'] == 'issue':
                print(f"   Problem: {result['problem'][:60]}...")
                print(f"   Solution: {result['solution'][:60]}...")
            else:
                print(f"   Case: {result['case_id']}")
            print()

        print()
